{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b61b3869-2097-41c6-9057-704a27937e67",
   "metadata": {},
   "source": [
    "Parts 1 and 2 were done separately and then combined in this file for user convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e39de3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1 - Parsing all news' URLs with tag of football club \"Spartak\" from Sports.ru website "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b0437809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98ac5e4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.sports.ru/football/club/spartak/news/',\n",
       " 'https://www.sports.ru/football/club/spartak/news/page2',\n",
       " 'https://www.sports.ru/football/club/spartak/news/page3',\n",
       " 'https://www.sports.ru/football/club/spartak/news/page4',\n",
       " 'https://www.sports.ru/football/club/spartak/news/page5',\n",
       " 'https://www.sports.ru/football/club/spartak/news/page6',\n",
       " 'https://www.sports.ru/football/club/spartak/news/page7',\n",
       " 'https://www.sports.ru/football/club/spartak/news/page8',\n",
       " 'https://www.sports.ru/football/club/spartak/news/page9',\n",
       " 'https://www.sports.ru/football/club/spartak/news/page10',\n",
       " 'https://www.sports.ru/football/club/spartak/news/page11',\n",
       " 'https://www.sports.ru/football/club/spartak/news/page12',\n",
       " 'https://www.sports.ru/football/club/spartak/news/page13',\n",
       " 'https://www.sports.ru/football/club/spartak/news/page14',\n",
       " 'https://www.sports.ru/football/club/spartak/news/page15',\n",
       " 'https://www.sports.ru/football/club/spartak/news/page16',\n",
       " 'https://www.sports.ru/football/club/spartak/news/page17',\n",
       " 'https://www.sports.ru/football/club/spartak/news/page18',\n",
       " 'https://www.sports.ru/football/club/spartak/news/page19',\n",
       " 'https://www.sports.ru/football/club/spartak/news/page20']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of links to parse\n",
    "\n",
    "Start_url='https://www.sports.ru/football/club/spartak/news/' # Start page\n",
    "URL_List = [] # empty list to store urls\n",
    "URL_List.append(Start_url) # Appending empty list with the url of Start page\n",
    "\n",
    "#loop for creating list of URLS\n",
    "for i in range(2, 21): #Limit of pages shown is 20 on the Sports.Ru website \n",
    "    New_Url=Start_url+f'page{i}' # Link format like in this example --> https://www.sports.ru/football/club/zenit/news/page3/\n",
    "    URL_List.append(New_Url) # Appending existing list with url of new page i\n",
    "\n",
    "URL_List #show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d5e684c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    URL        Date  \\\n",
      "0     /football/1116047335-sutormin-o-budushhem-vern...  2024-05-06   \n",
      "1     /football/1116047173-fakel-zenit-onlajn-transl...  2024-05-06   \n",
      "2     /football/1116042428-chempionat-rossii-krasnod...  2024-05-06   \n",
      "3     /football/1116047082-zarema-o-50-ochkax-u-lide...  2024-05-06   \n",
      "4     /football/1116046899-krugovoj-krasivo-zabil-za...  2024-05-06   \n",
      "...                                                 ...         ...   \n",
      "8095  /football/1115806035-mostovoj-ob-abaskale-v-re...  2024-01-30   \n",
      "8096  /football/1115805912-mostovoj-o-transfere-ugal...  2024-01-30   \n",
      "8097  /mediafootball/1115805809-cherdanczev-sprosil-...  2024-01-30   \n",
      "8098  /football/1115805810-gajich-ob-istorii-s-bolbo...  2024-01-30   \n",
      "8099  /football/1115805727-bilety-na-matchi-winline-...  2024-01-30   \n",
      "\n",
      "             Time                                         News Title  \\\n",
      "0     10:47:00+03  Сутормин о будущем: «Вернусь в «Зенит» по окон...   \n",
      "1     08:52:00+03  «Факел» – «Зенит». Онлайн-трансляция начнется ...   \n",
      "2     07:54:00+03  Чемпионат России. «Зенит» в гостях у «Факела»,...   \n",
      "3     07:36:00+03  Зарема о 50 очках у лидеров РПЛ после 27 игр: ...   \n",
      "4     00:25:00+03  Круговой красиво забил за «Зенит-2»: с лета по...   \n",
      "...           ...                                                ...   \n",
      "8095  16:25:00+03  Мостовой об Абаскале в рейтинге влиятельных лю...   \n",
      "8096  15:10:00+03  Мостовой о трансфере Угальде за 13+2 млн евро:...   \n",
      "8097  14:25:00+03  «Черданцев спросил: «У тебя были покровители? ...   \n",
      "8098  14:18:00+03  Гайич об истории с болбоем в матче со «Спартак...   \n",
      "8099  13:44:00+03  Билеты на матчи Winline Зимнего кубка РПЛ прод...   \n",
      "\n",
      "         News_id  \n",
      "0     1116047335  \n",
      "1     1116047173  \n",
      "2     1116042428  \n",
      "3     1116047082  \n",
      "4     1116046899  \n",
      "...          ...  \n",
      "8095  1115806035  \n",
      "8096  1115805912  \n",
      "8097  1115805809  \n",
      "8098  1115805810  \n",
      "8099  1115805727  \n",
      "\n",
      "[8100 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each page URL\n",
    "for url in URL_List:\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Find all <p> tags with class \"one_news\" and the specified data attributes\n",
    "        news_elements = soup.find_all(\"p\", class_=\"one_news\", attrs={\"data-news-dtime\": True, \"data-news-id\": True})\n",
    "\n",
    "        # Iterate over the found elements\n",
    "        for element in news_elements:\n",
    "            # Create a new temporary DataFrame to store the data\n",
    "            temp_df = pd.DataFrame(columns=columns, index=[0])\n",
    "\n",
    "            # Extract URL\n",
    "            news_url = element.find(\"a\", class_=\"short-text\").get(\"href\")\n",
    "            temp_df['URL'] = news_url\n",
    "\n",
    "            # Extract date and time\n",
    "            news_datetime = element.get(\"data-news-dtime\")\n",
    "            news_date, news_time = news_datetime.split(\" \")\n",
    "            temp_df['Date'] = news_date\n",
    "            temp_df['Time'] = news_time\n",
    "\n",
    "            # Extract news title\n",
    "            news_title = element.find(\"a\", class_=\"short-text\").get_text(strip=True)\n",
    "            temp_df['News Title'] = news_title\n",
    "\n",
    "            # Extract news ID\n",
    "            news_id = element.get(\"data-news-id\")\n",
    "            temp_df['News_id'] = news_id\n",
    "\n",
    "            # Concatenate the temporary DataFrame to the main DataFrame\n",
    "            News_Data = pd.concat([News_Data, temp_df], ignore_index=True)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(News_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "935b9c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to Excel\n",
    "News_Data.to_excel(\"news_data.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054f868d-b96b-4138-a10a-c58694d109c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2 - Parsing news content from collected list of URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b358b629-2f6b-4a47-9aee-952a3ca95f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d44e634-dbc7-47c6-8252-3a024f4c4b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Excel file into a DataFrame\n",
    "df = pd.read_excel(\"news_data_total.xlsx\")\n",
    "# Extract the column containing the URLs\n",
    "urls_column = df[\"URL\"]\n",
    "# Convert the column to a Python list\n",
    "URL_List = urls_column.tolist()\n",
    "URL_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437dbc00-1dfb-4e05-b547-5e6fbd0c823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data_and_update_df(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            \n",
    "            # Find the index of the row corresponding to the URL\n",
    "            index = df[df['URL'] == url].index[0]\n",
    "  \n",
    "            # Find the div with class 'news-item__tags-line'\n",
    "            tags_line_div = soup.find('div', class_='news-item__tags-line')\n",
    "            if tags_line_div:\n",
    "                # Extract text from all <a> tags within the div\n",
    "                all_news_tags = [tag.text for tag in tags_line_div.find_all('a')]\n",
    "                df.at[index, 'All news tags'] = all_news_tags\n",
    "                \n",
    "            # Find the div with class 'news-item__content js-mediator-article'\n",
    "            content_div = soup.find('div', class_='news-item__content js-mediator-article')\n",
    "            if content_div:\n",
    "                # Extract text from all <p> tags within the div\n",
    "                paragraphs = content_div.find_all('p')\n",
    "                news_content = '\\n'.join([p.get_text(strip=True) for p in paragraphs])\n",
    "                df.at[index, 'News Content'] = news_content\n",
    "\n",
    "            # Find the <p> element for 'Published by' and 'Source'\n",
    "            p_elements = soup.find_all('p')\n",
    "            for p_element in p_elements:\n",
    "                if \"Опубликовал\" in p_element.text:\n",
    "                    published_by = p_element.find('a').text\n",
    "                    df.at[index, 'Published by'] = published_by\n",
    "                if \"Источник\" in p_element.text:\n",
    "                    source = p_element.find('a').get('href')\n",
    "                    df.at[index, 'Source'] = source\n",
    "            \n",
    "            # Find the element with class 'comments-block-title'\n",
    "            comments_title_element = soup.find('div', class_='comments-block-title')\n",
    "            if comments_title_element:\n",
    "                # Extract text from the element\n",
    "                num_comments = comments_title_element.get_text(strip=True)\n",
    "                df.at[index, 'Number of comments'] = num_comments\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data from {url}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74ff6eb-57d4-4b30-91e8-a7b66fcca629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional dataframe to save original df unchanged\n",
    "df_add=df\n",
    "\n",
    "# add new columns to additional dataframe\n",
    "add_columns = ['All news tags', 'News COntent', 'Published by', 'Source', 'Number of comments']\n",
    "for col in add_columns:\n",
    "    df_add[col] = None\n",
    "    \n",
    "# Iterate through the URLs in the 'URL' column and fetch data for each URL\n",
    "for url in tqdm(df_add['URL'], desc=\"Processing URLs\"):\n",
    "    fetch_data_and_update_df(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ec10a7-ff76-43b3-b3a6-28b13a1a0ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add.to_excel('total total news.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a511a5-85d9-469e-b24e-05dd6a58e0a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
